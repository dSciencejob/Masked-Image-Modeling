# Intro

Self-supervised learning has managed to out perform supervised pre-training according to [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377). Can an asymmetric encoder-decoder architecture trained using masked random patches of images, manage to reconstruct them. Lets find out in [masked image modeling](/mim.ipynb), how the model performs when the images vary widely in terms of size and have a high level of noise.

# References

https://arxiv.org/abs/2111.06377

https://keras.io/examples/vision/masked_image_modeling/

https://www.happywhale.com
